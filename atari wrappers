#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep 14 19:57:58 2020

@author: hassan
"""

import numpy as np
from collections import deque
import gym
from gym import spaces
import cv2  # opencv-python

# this class is not that important 
# basically, it overwrites the reset function
# so, when we reset the environment, we dont get sent to the very 1st state
# instead, do the noop.action, which means do nothing for a random # steps, up to a max of 30 steps
# in this way, whenever we call reset on this wrapper environment, we get the very 1st state 
# appearing more random than it actually is
class NoopResetEnv(gym.Wrapper):
    def __init__(self, env, noop_max=30):
        """Sample initial states by taking random number of no-ops on reset.
        No-op is assumed to be action 0.
        """
        gym.Wrapper.__init__(self, env)
        self.noop_max = noop_max
        self.override_num_noops = None
        self.noop_action = 0
        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'

    def reset(self, **kwargs):
        """ Do no-op action for a number of steps in [1, noop_max]."""
        self.env.reset(**kwargs)
        if self.override_num_noops is not None:
            noops = self.override_num_noops
        else:
            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  # pylint: disable=E1101
        assert noops > 0
        obs = None
        for _ in range(noops):
            obs, _, done, _ = self.env.step(self.noop_action)
            if done:
                obs = self.env.reset(**kwargs)
        return obs

    def step(self, ac):
        return self.env.step(ac)

# also not that interesting
# has to do with the way some video games work
# ex: in some games, nothing happens until we hit the action button (Fire button in the code)
# so, this wrapper ensures that whenever we start a game, this happens automatically
# thus, the agent plays the game rather than having a frozen screen
class FireResetEnv(gym.Wrapper):
    def __init__(self, env):
        """Take action on reset for environments that are fixed until firing."""
        gym.Wrapper.__init__(self, env)
        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
        assert len(env.unwrapped.get_action_meanings()) >= 3

    def reset(self, **kwargs):
        self.env.reset(**kwargs)
        obs, _, done, _ = self.env.step(1)
        if done:
            self.env.reset(**kwargs)
        obs, _, done, _ = self.env.step(2)
        if done:
            self.env.reset(**kwargs)
        return obs

    def step(self, ac):
        return self.env.step(ac)

# This class is very interesting
# when we are using the raw environment for openAI gym, the episode is considered terminated after losing 5 lives
# (meaning, if we dont use this wrapper, the agent is allowed to die 5 times per episode)
# this wrapper overwrites that functionality and returns done flag set to true whenevr we lose a life
# so, in this implementation, one episode means playing the gaeme with 1 life
# it is related to NoopResetEnv class, which is used for the purpose of adding variability to the initial state
# this one adds further variability to the initial state, bcs now the beginning of an episode is whatever the game looks like
# when you lost the last life, rather than a complete reset which would always return you to the same state
class EpisodicLifeEnv(gym.Wrapper):
    def __init__(self, env):
        """Make end-of-life == end-of-episode, but only reset on true game over.
        Done by DeepMind for the DQN and co. since it helps value estimation.
        """
        gym.Wrapper.__init__(self, env)
        # 1st, we will have 2 attributes: self.lives and self.was_real_done
        self.lives = 0
        self.was_real_done = True
    
    # next, the step function, which is the main thing we ae overwriting
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        # 1st, we set self.was_real_done to whatever the done variable is, that we get back from the inner step function
        # so, this is the true value of done
        self.was_real_done = done
        # ..check current lives, make loss of life terminal,
        # ..then update lives to handle bonus lives
        # then, we call self.env.unwrapped.ale.lives(), to get the # lives left in the game
        # (to do that, we have to know about the openAI gym, that it has attribute unwrapped, and attribute ale, and function lives(), to get # lives)
        lives = self.env.unwrapped.ale.lives()
        if lives < self.lives and lives > 0:
            # for Qbert sometimes we stay in lives == 0 condtion for a few frames
            # so its important to keep lives > 0, so that we only reset once
            # the environment advertises done.
            done = True
        self.lives = lives
        return obs, reward, done, info

    # the reset function is also important to overwrite
    # bcs, when we lose a life, its considered a terminal state, and we return done=true
    # at that point, the agent will call reset (but we dont want to do a true reset, bcs that would put us back to the beginning of the game with 5 lives)
    # whereas, we want to continue until we have no lives left
    # so, we only call the inner reset if self.was_real_done is true, otherwise we do a noop step and set lives to current # lives
    def reset(self, **kwargs):
        """Reset only when lives are exhausted.
        This way all states are still reachable even though lives are episodic,
        and the learner need not know about any of this behind-the-scenes.
        """
        if self.was_real_done:
            obs = self.env.reset(**kwargs)
        else:
            # no-op step to advance from terminal/lost life state
            obs, _, _, _ = self.env.step(0)
        self.lives = self.env.unwrapped.ale.lives()
        return obs

# to understand this class, we have to know how openAI gym environemnts work
# ex: in Breakut, there is environment Breaout-v0 and BreakoutNoFrameskip-v4
# v0 introduces randomness to # actions performed to let the agent react to whats happening on screen. also, in v0, when we do an action, it might be completely ignored
# v4, we get to see every frame, and the agent performs the same action only once. plus, the actions deterministic rather than randomly ignored. we can control frameskipping if we wanna include it
class MaxAndSkipEnv(gym.Wrapper):
    def __init__(self, env, skip=4):
        """Return only every `skip`-th frame"""
        gym.Wrapper.__init__(self, env)
        # most recent raw observations (for max pooling across time steps)
        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype='uint8')
        self._skip = skip
    
    # the step function: its gonna do the same thing over 4 frames deterministically
    # so, it will always be 4 frames, and always performing the same action for each frame
    # furthermore, the observation that's returned will be the max over the last 2 frames
    # this is udseful, in case the ball randomly disappears in some frames
    # this will combine the 2 frames together, taking the pixel from whichever frame had the max value
    def step(self, action):
        """Repeat action, sum reward, and max over last observations."""
        total_reward = 0.0
        done = None
        for i in range(self._skip):
            obs, reward, done, info = self.env.step(action)
            if i == self._skip - 2:
                self._obs_buffer[0] = obs
            if i == self._skip - 1:
                self._obs_buffer[1] = obs
            total_reward += reward
            if done:
                break
        # Note that the observation on the done=True frame
        # doesn't matter
        max_frame = self._obs_buffer.max(axis=0)

        return max_frame, total_reward, done, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

# this class bins the rewards so it can only be 1 of 3 values (-1, +1, 0)
# if we get + rewards its sign +!, if - reward its a sign -1
# this is useful in RL, bcs we aften have problems with gradients being too large and going in the wrong direction
# by ensuring that the reward has a magnitude of 1 or 0, we can limit the step size of each iteration
# we will notice something strange when training, which is that the score seems too low
# this is only artificial, bcs we are clipping the reward and then accumulating that clipped reward
# this is related to the previous wrapper we discussed, which repeats the same actions over 4 frames and treats it like 1 step
# so, in those 4 frames, we might receive multiple rewards, but this wrapper will reduce it to 1
# when plaing the game the game, the rewards are much higher than in training
class ClipRewardEnv(gym.RewardWrapper):
    def reward(self, reward):
        """Bin reward to {+1, 0, -1} by its sign."""
        return np.sign(reward)


# class WarpFrame(gym.ObservationWrapper):
#     def __init__(self, env):
#         """Warp frames to 84x84 as done in the Nature paper and later work."""
#         gym.ObservationWrapper.__init__(self, env)
#         self.width = 84
#         self.height = 84
#         self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1))

#     def _observation(self, frame):
#         frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
#         frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
#         return frame[:, :, None]

# this wrapper class, 1st it grey scales the image, then resizes the image to 84 x 84 (it uses opencv)
class WarpFrame(gym.ObservationWrapper):
    def __init__(self, env, width=84, height=84, grayscale=True):
        """Warp frames to 84x84 as done in the Nature paper and later work."""
        gym.ObservationWrapper.__init__(self, env)
        self.width = width
        self.height = height
        self.grayscale = grayscale
        if self.grayscale:
            self.observation_space = spaces.Box(low=0, high=255,
                shape=(self.height, self.width, 1), dtype=np.uint8)
        else:
            self.observation_space = spaces.Box(low=0, high=255,
                shape=(self.height, self.width, 3), dtype=np.uint8)

    def observation(self, frame):
        if self.grayscale:
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
        if self.grayscale:
            frame = np.expand_dims(frame, -1)
        return frame

# this class doesnt modify the behavior of the environment
# it exists only for computational efficiency (memory efficiency)
# each state is made up of 4 consecutive frames
# but the next state is made up of 3 of the most recent frames + the newest frame
# (thus, the previous state and next state have 3 frames in common. and we wouldnt want to duplicate them)
# so, the idea behind this wrapper is to avoid duplicate frames
class FrameStack(gym.Wrapper):
    def __init__(self, env, k):
        """Stack k last frames.
        Returns lazy array, which is much more memory efficient.
        See Also
        --------
        baselines.common.atari_wrappers.LazyFrames
        """
        gym.Wrapper.__init__(self, env)
        self.k = k
        self.frames = deque([], maxlen=k)
        shp = env.observation_space.shape
        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k))

    def reset(self):
        ob = self.env.reset()
        for _ in range(self.k):
            self.frames.append(ob)
        return self._get_ob()

    def step(self, action):
        ob, reward, done, info = self.env.step(action)
        self.frames.append(ob)
        return self._get_ob(), reward, done, info

    def _get_ob(self):
        assert len(self.frames) == self.k
        return LazyFrames(list(self.frames))


class LazyFrames:
    def __init__(self, frames):
        """This object ensures that common frames between the observations are only stored once.
        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay
        buffers.
        This object should only be converted to numpy array before being passed to the model.
        You'd not believe how complex the previous solution was."""
        self._frames = frames

    def __array__(self, dtype=None):
        out = np.concatenate(self._frames, axis=2)
        if dtype is not None:
            out = out.astype(dtype)
        return out

# makes an environment, but also adds some wrappers needed for atari environments
# we wanna make sure we do 3 things:
# 1. make sure we have the NoFrameSkip version of the environment
# 2. apply the NoopResetEnv wrapper
# 3. apply the MaxAndSkipEnv wrapper
def make_atari(env_id):
    env = gym.make(env_id)
    assert 'NoFrameskip' in env.spec.id
    env = NoopResetEnv(env, noop_max=30)
    env = MaxAndSkipEnv(env, skip=4)
    return env

# this wrapper, adds more wrappers to make the environment behave as described in deep mind papers
# so, this applies all other previous wrappers
def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False):
    """Configure environment for DeepMind-style Atari.
    """
    if episode_life:
        env = EpisodicLifeEnv(env)

    if 'FIRE' in env.unwrapped.get_action_meanings():
        env = FireResetEnv(env)
    env = WarpFrame(env)

    if clip_rewards:
        env = ClipRewardEnv(env)

    if frame_stack:
        env = FrameStack(env, 4)

    return env

# this class is not in openAI baselines
# since the game ends after losing 1 life, the reward is only known b/w each life
# we wanna know the total reward from a true reset of the game down to when we have no lives left
# ofc, this is not the true score of the game, bcs the rewards are clipped 
class Monitor(gym.Wrapper):
    def __init__(self, env, rank=0):
        gym.Wrapper.__init__(self, env=env)
        self.rank = rank
        self.rewards = []
        self.total_reward = []
        self.summaries_dict = {'reward': 0, 'episode_length': 0, 'total_reward': 0, 'total_episode_length': 0}
        env = self.env
        while True:
            if hasattr(env, 'was_real_done'):
                self.episodic_env = env
            if not hasattr(env, 'env'):
                break
            env = env.env

    def reset(self):
        self.summaries_dict['reward'] = -1
        self.summaries_dict['episode_length'] = -1
        self.summaries_dict['total_reward'] = -1
        self.summaries_dict['total_episode_length'] = -1
        self.rewards = []
        env = self.env
        if self.episodic_env.was_real_done:
            self.summaries_dict['total_reward'] = -1
            self.summaries_dict['total_episode_length'] = -1
            self.total_reward = []
        return self.env.reset()

    def step(self, action):
        observation, reward, done, info = self.env.step(action)
        self.rewards.append(reward)
        self.total_reward.append(reward)
        if done:
            # print("Done! R = %s, N = %s" % (sum(self.rewards), len(self.rewards)))
            self.summaries_dict['reward'] = sum(self.rewards)
            self.summaries_dict['episode_length'] = len(self.rewards)

            if self.episodic_env.was_real_done:
                self.summaries_dict['total_reward'] = sum(self.total_reward)
                self.summaries_dict['total_episode_length'] = len(self.total_reward)
        info = self.summaries_dict.copy() # otherwise it will be overwritten
        # if done:
        #     print("info:", info)
        return observation, reward, done, info
